---
title: "TTHAA_Test"
author: "RLionheart & ABoysen"
date: "12/02/2020"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: inline
---

------------------------------------------------------------------------------------------------------
*Remember to change the "title" at the top of the page to your project. Below, add a short description of your project*
THAA_Test for Angie, related to the Gradients project.

------------------------------------------------------------------------------------------------------
This markdown script controls the targeted pipeline for targeted data. 
The script handles data from both QE and TQS instruments, 
as well as data processed by Skyline and MSDial metabolomics processing platforms.

For the THAA_Test project, the four major sections have been modified to remove the B-MIS
section and the Internal Standard / Response Factor quantification. 

Section I: Import and cleaning/rearranging of data.
Section II: Quality control using user-defined parameters.
Section III: _B-MIS, not included in this analysis_
Section III: Quantifying peak area using standard curves.

In order to run this script, you will need the following items:
1. The instrument output in csv format from either MSDial or Skyline.
2. A sample key from the run, which should include information such as the Column, the Bio Normalization,
the Volume Filtered, and the Dilution Factor. An example of this is included in the data_extras/ subdirectory.


```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(anytime)
library(ggpmisc) # Additional library for standard curve calculation
library(rlist)
library(tidyr)
library(tidyverse)
options(scipen=999)
currentDate <- Sys.Date()

source("src/Functions.R")

processed.folder <- "data_processed" 
intermediate.folder <- "data_intermediate"
extras.folder <- "data_extras"
raw.folder <- "data_raw"
figures.folder <- "figures"

dir.create(file.path(".", processed.folder))  
dir.create(file.path(".", intermediate.folder))  
dir.create(file.path(".", extras.folder))
dir.create(file.path(".", raw.folder))
dir.create(file.path(".", figures.folder))  

if (length(dir(path = "data_processed/")) == 0) {
  cat("\nData_processed subdirectory has been created and is ready for new data.")
} else {
  cat("Warning: some or all of the subdirectories are not empty. Empty contents before continuing.\n")
}
```


If you would like to empty the data_processed/, data_intermediate, and figures/ subdirectories, run the following code.
```{r, include = TRUE}
toClear <- c("data_processed/", "data_intermediate", "figures/")
f <- list.files(toClear, include.dirs = F, full.names = T, recursive = T)
file.remove(f)

print("Subdirectories emptied.")
```

----------------------------------------------------------------------------------------------------------------------------
Section I: Import all MSDial files that have been split by Area, Mass/charge (Mz), Retention Time (RT), and Signal to Noise (SN). Or, import your single Skyline file in long format.

For MSDial data:
Set header, filter unknowns.
Change variable classes from character/factor to numeric, according to column.
Rearrange the dataframes and combine to a single frame in long format.
Standardize dataset by removing "Ingalls_" prefixes from compound names, and removing the syntactically correct "X" from Replicates.

For Skyline data:
Ensure all columns are the appropriate class (numeric, character, etc.)

***
Inputs: 
"data_raw/*file.pattern*.csv
Outputs: 
"data_intermediates/*software.pattern*_combined_*file.pattern*_*DATE*.csv"
***

*User action required*
Comment or uncomment the file.pattern, software.pattern, instrument.pattern, and matching.pattern required for your files.
The file.pattern is the variable that will help the program pick out the correct file from the data_raw folder, and may need to be changed by the user according to the filename. 
The instrument.pattern refers to which instrument was used to analyze the data.

The matching.pattern and software.pattern variables should not be changed by the user, only commented or uncommented.
```{r Pattern matching, include = TRUE}
### Choose software pattern ###
# MSDial
# software.pattern <- "MSDial"

# Skyline
software.pattern <- "Skyline"

### Choose instrument pattern ###
# QE
instrument.pattern <- "QE"

# TQS
#instrument.pattern <- "TQS"

### Create file pattern ###
file.pattern <- "GBT-Fate_THAA_T0"
# file.pattern <- "HILIC"

### Choose matching pattern ###

# Cyano 
matching.pattern <- "RP.Cyano"
# matching.pattern <- "pos|neg"

print(paste("Your software type is:", software.pattern))
print(paste("The instrument used in this run is:", instrument.pattern))
print(paste("Your file matching pattern is:", file.pattern))
print(paste("Your run type is:", matching.pattern))
```


```{r Imports, include = TRUE}
source("src/File_Import.R")

print("Required files imported.")
```

This section is specific to the THAA_Test run.
Removes the "-2" standard run from the instrument output.
```{r Remove extraneous standard curves, include = TRUE}
GBT.Fate_THAA_T0.Tlong <- GBT.Fate_THAA_T0.Tlong %>%
  filter(!str_detect(Replicate.Name, "-1")) 
```

*User action required*
This step changes depending on whether you are using Skyline or MSDial.

Enter the existing filenames of your run. The above code assigns the variables in R to their filename in the directory, so if your positive Area file is "data_processed/PositiveArea.csv", it will be imported to this code as PositiveArea. Those files need to be reassigned to Area.positive so the rest of the code will know which files to edit for the pipeline steps.

Comment or uncomment the block of variable names appropriate for your run.

```{r Dataset reassignment, include = TRUE}
# Comment out the run not being used.

## Skyline Cyano variables:
skyline.RP.Cyano <- GBT.Fate_THAA_T0.Tlong

## Skyline HILIC variables: 
# skyline.HILIC.neg <- X181116_HILIC_neg_MESO.SCOPE_HRM
# skyline.HILIC.pos <- X181116_HILIC_pos_MESO.SCOPE_HRM

## MSDial Cyano variables: 
# Area.RP.Cyano <- Area_CYANO_EddyTransect
# Mz.RP.Cyano   <- Mz_CYANO_EddyTransect
# RT.RP.Cyano   <- RT_CYANO_EddyTransect
# SN.RP.Cyano   <- SN_CYANO_EddyTransect

## MSDial HILIC variables: 
# Area.positive <- Area_HILICPos_Example
# Mz.positive   <- Mz_HILICPos_Example
# RT.positive   <- RT_HILICPos_Example
# SN.positive   <- SN_HILICPos_Example
# 
# Area.negative <- Area_HILICNeg_Example
# Mz.negative   <- Mz_HILICNeg_Example
# RT.negative   <- RT_HILICNeg_Example
# SN.negative   <- SN_HILICNeg_Example

print(paste(file.pattern, instrument.pattern, software.pattern, "variables assigned."))
```

Check if dataset is MSDial, rearrange if so, and export.
```{r Dataset rearrangement, include = TRUE}
if (software.pattern == "MSDial") {
  source("src/MSDial_Rearrange.R")
  print("Data rearrange complete.")
} else {
  source("src/Skyline_Rearrange.R")
  print("This is a Skyline datafile. Exporting file to data_intermediate.")
}

# Clear environment
rm(list = setdiff(ls()[!ls() %in% c("file.pattern", "currentDate", "instrument.pattern", "software.pattern")], lsf.str()))
```

--------------------------------------------------------------

Section II: Quality Control and flagging of problematic peaks.

In the Quality Control Step for QE files:
Import files.
Identify run types and check if all are present (blk, smp, std, poo).
Create a table of standard retention times (RT) for comparison.
Create a table of areas from blank runs for comparison.
Flag peaks in the dataset that fall outside of user-defined bounds.
Add parameter values to the top of the final file and save to the data_processed/ folder.

Additional TQS step:
Create standard ion ratio table for comparison.

***
Inputs: 
"data_intermediate/*software.pattern*_combined_*file.pattern*_*DATE*.csv"

Outputs: 
"data_intermediate/*software.pattern*_*instrument.pattern*_RT.table_*DATE*.csv"
"data_intermediate/*software.pattern*_*instrument.pattern*_SN.table_*DATE*.csv"
"data_intermediate/*software.pattern*_*instrument.pattern*_area.table_*DATE*.csv"
"data_intermediate/*software.pattern*_*instrument.pattern*_final.table_*DATE*.csv"
"data_intermediate/*software.pattern*_*instrument.pattern*_blank.table_*DATE*.csv"
"data_processed/*instrument.pattern*_QC_Output_*file.pattern*_*DATE*.csv"
Additional TQS Output:
"data_processed/*instrument.pattern*_IR.table_*DATE*.csv"
***

*User action required*
Define parameters for quality control. These act as comparison for filtering out data.
The numbers will change depending on whether you are analyzing HILIC vs Cyano data, or if you are measuring TQS vs QE data.

```{r QC parameters, include = TRUE}
# QE + TQS QC parameters

area.min   <- 5000 # HILIC - 1000, Cyano - 5000
RT.flex    <- 0.2 # HILIC +/- 0.4 min, Cyano +/- 0.2 min 
blk.thresh <- 0.2 # HILIC +/- 0.3, Cyano +/- 0.2
SN.min     <- 4 # HILIC - 4, Cyano - 4
height.min <- 1000
height.max <- 5.0e8


# Additional QC parameters for Skyline TQS 
# Comment this out when using MSDial
# area.max <- 1.0e8
# IR.flex  <- 0.3
# ppm.flex <- 7

print("Parameter values assigned.")
```

Run Quality Control and export.
```{r MSDial and Skyline QC, include=TRUE}
if (software.pattern == "MSDial") {
  source("src/MSDial_QC.R")
} else {
  source("src/Skyline_QC.R")  
}
```

Inspect the blank.table, final.table, and RT.table values, which currently exist in the environment.
Ensure that they look normal before proceeding to clear the environment in the next step.
```{r, include = TRUE}
currentDate <- Sys.Date()
csvFileName <- paste("data_processed/", software.pattern, "_", instrument.pattern,
                     "_QC_Output_", file.pattern, "_", currentDate, ".csv", sep = "")

tables <- grep("table", names(.GlobalEnv), value = TRUE, ignore.case = TRUE)
tablelist <- do.call("list", mget(tables))

# Write intermediate data
invisible(lapply(tables, 
                 function(x) write.csv(get(x), file=paste("data_intermediate/",
                                                            software.pattern, "_",
                                                          instrument.pattern,
                                                            "_", x, "_", currentDate,
                                                          ".csv", sep = ""))))
# Write final data
write.csv(final.table, csvFileName, row.names = FALSE)

print(paste(tables, "saved to data/intermediate"))

rm(list = setdiff(ls()[!ls() %in% c("file.pattern")], lsf.str()))
```

--------------------------------------------------------------

Section III: Best-Matched Internal Standard (B-MIS)

Because internal standards were not added to this run, and there are no pooled samples, 
raw peak areas were used for this analysis.
--------------------------------------------------------------
***
############################################3
Section IV: Convert from peak area to umol/vial.
##############################################333
In the quantify step:
Subtract the blank runs from the standard curves.
Isolate the standard curve runs and extract slope and interval values for each compound.
Use those values to calculate concentrations for all samples. 
***

```{r, Import files and set variables}
source("src/Functions.R")
spikes <- c("0uM", "0.5uM", "1.0uM", "2.5uM")
vial_types <- c("Durapore", "GF75", "Omnipore", "Vial")

GBT_StdCurves <- read.csv("data_raw/GBT-Fate_THAA_T0-Tlong.csv") %>%
  select(Replicate.Name, Precursor.Ion.Name, Area) %>%
  mutate(runtype = ifelse(str_detect(Replicate.Name, "Blk"), "Blank", "Sample")) %>%
  filter(!str_detect(Replicate.Name, "-1"))
```


```{r, Subtract the blanks from the standard areas}
GBT_StdCurves_BlksSub <- GBT_StdCurves %>%
  mutate(Replicate.Name = ifelse(runtype == "Blank", 
                                 substr(Replicate.Name, 1, nchar(Replicate.Name)-2), Replicate.Name)) %>%
  group_by(Precursor.Ion.Name, runtype) %>%
  mutate(Area = ifelse(str_detect(Replicate.Name, "Blk"), 
                       round(mean(Area, na.rm = TRUE)), Area)) %>%
  unique() %>%
  ungroup() %>%
  group_by(Precursor.Ion.Name) %>%
  mutate(blankArea = Area[which(runtype == "Blank")]) %>%
  mutate(Area_noBlank = Area - blankArea)
```

```{r, Isolate Concentrations}
GBT_StdCurves_wConcentration <- GBT_StdCurves_BlksSub %>%
  separate(Replicate.Name, into = c("Date", "run", "TempConcentration", "SampID", "Replicate"), sep = "_") %>%
  filter(TempConcentration %in% spikes) %>%
  mutate(TempConcentration = substr(TempConcentration, 1, nchar(TempConcentration)-2),
         TempConcentration = as.numeric(TempConcentration)) %>%
  mutate(Concentration_uM = TempConcentration) %>%
  unite(Date, run, TempConcentration, SampID, Replicate, col = "Replicate.Name")
```

```{r, Plot standard curves}
GBT_StdCurves_Plot <- ggplot(GBT_StdCurves_wConcentration, 
                             aes(x=Concentration_uM, y=Area_noBlank, group = Precursor.Ion.Name)) +
  facet_wrap(~Precursor.Ion.Name) +
  geom_point() + 
  geom_smooth(method=lm, se=TRUE, fullrange=TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5)) +
  stat_poly_eq(aes(label=..eq.label..),
               geom="label", alpha=0.33, formula=(y ~ x),
               label.y = 0.9 * max(GBT_StdCurves_wConcentration$Area_noBlank), 
               label.x = 0.5 * max(GBT_StdCurves_wConcentration$Concentration_uM),
               size = 2.5, parse=TRUE) +
  theme(text=element_text(size=10)) 
print(GBT_StdCurves_Plot)

currentDate <- Sys.Date()
ggsave(filename=paste("figures/GBT_StdCurvesPlot_", currentDate, ".png", sep = ""), plot=GBT_StdCurves_Plot)

```

```{r, Extract slope values}
Slope.Values <- GBT_StdCurves_wConcentration %>% 
  group_by(Precursor.Ion.Name) %>% 
  do({
    mod = lm(Area_noBlank ~ Concentration_uM, data = .)
    data.frame(Intercept = coef(mod)[1],
               Slope = coef(mod)[2])
  })
```

Calculate concentrations using the slope and intercept data.
```{r, Join slope and intercept with data}
All.Standards <- GBT_StdCurves_wConcentration %>%
  left_join(Slope.Values) %>%
  select(Replicate.Name, Precursor.Ion.Name, Area_noBlank, Concentration_uM, Intercept, Slope)

Final.Concentrations <- read.csv("data_raw/GBT-Fate_THAA_T0-Tlong.csv") %>%
  select(Replicate.Name, Precursor.Ion.Name, Area) %>%
  mutate(runtype = ifelse(str_detect(Replicate.Name, "Blk"), "Blank", "Sample")) %>%
  filter(!str_detect(Replicate.Name, "-1"))%>%
  mutate(Replicate.Name = ifelse(runtype == "Blank", 
                                 substr(Replicate.Name, 1, nchar(Replicate.Name)-2), Replicate.Name)) %>%
  group_by(Precursor.Ion.Name, runtype) %>%
  mutate(Area = ifelse(str_detect(Replicate.Name, "Blk"), 
                       round(mean(Area, na.rm = TRUE)), Area)) %>%
  unique() %>%
  ungroup() %>%
  group_by(Precursor.Ion.Name) %>%
  mutate(blankArea = Area[which(runtype == "Blank")]) %>%
  mutate(Area_noBlank = Area - blankArea) %>%
  left_join(All.Standards %>% select(Precursor.Ion.Name, Slope, Intercept)) %>%
  unique() %>%
  group_by(Precursor.Ion.Name) %>%
  mutate(Calculated.Concentration = (Area_noBlank - Intercept) / Slope)
```
```{r, Second concentration calculation using only slope}
## Calculation using only the slope 
Final.Concentrations.SlopeCalc <- Final.Concentrations %>%
  select(Replicate.Name, Precursor.Ion.Name, Area, Area_noBlank, Slope, Intercept, Calculated.Concentration) %>%
  filter(!str_detect(Replicate.Name, "A-1|Blk")) %>%
  mutate(Runtype = ifelse(str_detect(Replicate.Name, "A-2"), "StandardCurve", "Sample"),
         Conc.Type = ifelse(Runtype == "Standard"|str_detect(Replicate.Name, "_0uM_"), 0, NA)) %>%
  group_by(Precursor.Ion.Name) %>%
  mutate(Zero.Concentration.Area = Area_noBlank[which(Conc.Type == 0)],
         # (x2 - x1) = (y2 - y1) / slope
         NewConc = (Area_noBlank - Zero.Concentration.Area)/(Slope),
         Conc.Diff = abs(NewConc - Calculated.Concentration),
         RealCon = Area_noBlank/Slope)
```

```{r, Save files}
currentDate <- Sys.Date()
csvFileName <- paste("data_processed/Quantified_Output_", file.pattern, "_", currentDate, ".csv", sep = "")
csvFileName_slopeonly <- paste("data_processed/Quantified_Output_slopeonly_", file.pattern, "_", currentDate, ".csv", sep = "")


# Write final data
write.csv(Final.Concentrations, csvFileName, row.names = FALSE)
write.csv(Final.Concentrations.SlopeCalc, csvFileName_slopeonly, row.names = FALSE)
```

Clear environment.
```{r, include=FALSE}
rm(list = ls())
```